{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.sql import read_sql\n",
    "import os\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory paths where we saved the tweets\n",
    "train = \"../data/train.csv\"\n",
    "test = '../data/test.csv'\n",
    "#'../data/train.csv', encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for another implementation see https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_data(filename, test = False):\n",
    "    \n",
    "    data = pd.read_csv(filename)\n",
    "    if test:\n",
    "        data = data[['id','text']]\n",
    "        \n",
    "    else:\n",
    "        data = data[['text','target']]\n",
    "        \n",
    "        \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets = load_data(train)\n",
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and validation \n",
    "from sklearn.model_selection import train_test_split\n",
    "training_data, test_data = train_test_split(train_tweets,\n",
    "                                     random_state = 42,\n",
    "                                     stratify = train_tweets[['target']],\n",
    "                                     test_size = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use an iterator class that we will feed into python for training\n",
    "# Next step is define a class that takes the text and labels, tokenizes the text \n",
    "# using the bert tokenizer, converts tokens to ids, pads the sentences to make sure they are the same\n",
    "# size as the model allows; if they are longer it trims them else it pads them with 0.\n",
    "# finallly feeds themn to the classifier.\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "config = BertConfig(num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_seq_length = 280\n",
    "\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, text_column, label_column):\n",
    "        \n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "    \n",
    "        #self.label_column = pd.Categorical(pd.factorize(label_column)[0])\n",
    "    \n",
    "        # define the label column and transform it to list\n",
    "        self.label_column = label_column.tolist()\n",
    "    \n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        tokenized_text = tokenizer.tokenize(self.text_column[index])\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "\n",
    "        # check for the sequence lenght taking into consideration the \n",
    "        # fact that we need to include the SEP special token and the CLS special\n",
    "        # tokens. \n",
    "        if len(tokenized_text) > max_seq_length - 2:\n",
    "            tokenized_text = tokenized_text[0:(max_seq_length - 2)]\n",
    "\n",
    "        # We add the CLS token at the beginning of the tokenized sequence and the \n",
    "        # SEP token at the end.\n",
    "        tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "\n",
    "        # convert the inputs to ids (dict looking)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        # We define the size of the input mask to correspon to the lenght of the inputs\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "\n",
    "\n",
    "        #attention_masks.append(seq_mask) \n",
    "\n",
    "        #input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "        # \n",
    "        input_ids += padding\n",
    "\n",
    "        #input_mask += padding\n",
    "\n",
    "        attention_masks = [1 if x>0 else 0 for x in input_ids] \n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(attention_masks) == max_seq_length\n",
    "\n",
    "        #print(ids_review)\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "\n",
    "        labels = self.label_column[index] \n",
    "\n",
    "        #list_of_labels = [torch.from_numpy(np.array(labels)).squeeze(0)]\n",
    "        list_of_labels = torch.tensor(labels)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "        return input_ids, list_of_labels, attention_masks\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# create a class to process the traininga and test data\n",
    "training_data = Data_Processing(training_data['text'], training_data['target'])\n",
    "\n",
    "test_data =  Data_Processing(test_data['text'], test_data['target'])\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=10),\n",
    "                    'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                 'val':len(test_data)}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101,  2444, 14409,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  1005,  ...,     0,     0,     0],\n",
       "         [  101,  1001,  1052,  ...,     0,     0,     0],\n",
       "         [  101,  1030,  9915,  ...,     0,     0,     0]]),\n",
       " tensor([0, 0, 0, 0]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = next(iter(dataloaders_dict.get('train')))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../data/bert_tweet_language_finetune_model.pth')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",config = config, state_dict = state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# define the metrics to evaluate\n",
    "def log_metrics(y_pred, y_true):\n",
    "    print('Accuracy:', accuracy_score(y_true,y_pred))\n",
    "    #print('MCC:', matthews_corrcoef(y_true,y_pred))\n",
    "    print('F1 score:', f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level = logging.INFO, filename ='bert_tweet_classifier_280_finetune.txt', filemode ='w', \n",
    "                   format = '%(name)s -%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "from torch.optim import lr_scheduler, AdamW\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "lrmain = 2e-5\n",
    "\n",
    "optim1 = optim.AdamW(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "   ])\n",
    "\n",
    "optimizer_ft = optim1\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "#import torch.nn.functional as F\n",
    "import copy \n",
    "import time\n",
    "number_steps = 20\n",
    "print(number_steps)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=3):\n",
    "    best_eval_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        #zero the model gradients\n",
    "        model.zero_grad()\n",
    "        print(f'starting epoch {epoch+1} out of {num_epochs}')\n",
    "        training_loss = []\n",
    "        training_accuracy = []\n",
    "        val_loss = []\n",
    "        val_accuracy = []\n",
    "        outputs_ = []\n",
    "        labels_ = []\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # track number of batches and number of iterations\n",
    "        counter = 0\n",
    "        iterations = 0\n",
    "\n",
    "        # Iterate over data, feeding inputs, attention masks and labels\n",
    "        model.train()\n",
    "        for i, (inputs, label, attention_mask) in enumerate(dataloaders_dict['train']):\n",
    "            # add a counter that will register how many examples we have fed to the\n",
    "            # model\n",
    "            counter+= batch_size\n",
    "            iterations+=1\n",
    "            # move the sequences, labels and masks to the GPU\n",
    "            inputs = inputs.to(device) \n",
    "            label = label.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            # feed the sequences to the model, specifying the attention mask\n",
    "            loss, outputs = model(inputs, attention_mask=attention_mask, labels=label)\n",
    "            outputs1 = outputs\n",
    "            \n",
    "            # feed the logits returned by the model to the softmax to classify the function\n",
    "            outputs = F.softmax(outputs,dim=1)\n",
    "\n",
    "            # calculate the loss function, squeeze the labels so their shapes are compatible\n",
    "            loss_manual = criterion(outputs, label.squeeze(0))\n",
    "            \n",
    "            # divide the loss by the number of steps\n",
    "            loss_reg = loss / number_steps \n",
    "\n",
    "            #add the loss to the epoch loss\n",
    "            epoch_loss += loss_reg\n",
    "            training_loss.append(loss_reg)\n",
    "\n",
    "            loss.backward()\n",
    "      \n",
    "            # accumulate gradients and update every x batches\n",
    "            if (i+1) % number_steps == 0:\n",
    "                \n",
    "                optimizer.step()                            # Now we can do an optimizer step\n",
    "                model.zero_grad()                           # Reset gradients tensors\n",
    "      \n",
    "            # only present the information \n",
    "            if counter%1000 == 0:\n",
    "                # get the predictions and the true labels out of the GPU\n",
    "                preds1 = torch.argmax(outputs,dim=1).cpu().data.numpy()\n",
    "                true1 = np.array(label.squeeze(0).cpu().data.numpy())\n",
    "         \n",
    "        # get the accurary score\n",
    "                training_accuracy.append(accuracy_score(preds1,true1))\n",
    "            \n",
    "                print(f'current training loss is {epoch_loss/iterations} and accuracy is {np.mean(training_accuracy):,.2%}')\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            counter_val = 0\n",
    "            iterations_val = 0\n",
    "            \n",
    "            for i, (inputs, label, attention_mask) in enumerate(dataloaders_dict['val']):\n",
    "                counter_val += batch_size\n",
    "                iterations_val += 1\n",
    "      \n",
    "                # move the sequences, labels and masks to the GPU\n",
    "                inputs = inputs.to(device) \n",
    "                label = label.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "          \n",
    "              # feed the sequences to the model, specifying the attention mask\n",
    "                loss_eval, outputs = model(inputs, attention_mask=attention_mask, labels = label)\n",
    "\n",
    "              # feed the logits returned by the model to the softmax to classify the function\n",
    "                outputs = F.softmax(outputs,dim=1)\n",
    "                \n",
    "              # calculate the loss function, squeeze the labels so their shapes are compatible\n",
    "                loss_eval_manual = criterion(outputs, label.squeeze(0))\n",
    "                val_loss.append(loss_eval)\n",
    "                \n",
    "                preds1 = torch.argmax(outputs,dim=1).cpu().data.numpy()\n",
    "                true1 = np.array(label.squeeze(0).cpu().data.numpy())\n",
    "      \n",
    "                # get the accurary score\n",
    "                val_accuracy.append(accuracy_score(preds1,true1))\n",
    "\n",
    "                if counter_val % 1000 == 0:\n",
    "                    # get the predictions and the true labels out of the GPU for validation\n",
    "                    \n",
    "                    print(f'current validation loss is {np.sum(val_loss)/iterations_val} and accuracy is {np.mean(val_accuracy):,.2%}')\n",
    "                              \n",
    "        print(f'For epoch {epoch+1} training loss is {np.sum(training_loss)/iterations}, \\\n",
    "        training accuracy is {np.mean(training_accuracy):,.2%}, Validation \\\n",
    "        loss is {np.sum(val_loss)/iterations_val} and validation accuracy is {np.mean(val_accuracy):,.2%}')\n",
    "        eval_acc = np.mean(val_accuracy)\n",
    "        if eval_acc >= best_eval_acc:\n",
    "            best_eval_acc = eval_acc\n",
    "            print(f'saving the model with validation accuracy of {eval_acc:,.2%} ')\n",
    "            torch.save(model.state_dict(), 'bert_tweet_classification_state_dict_280_finetuned.pth')\n",
    "            #torch.save(optimizer_ft.state_dict(), \n",
    "            #           'bert_tweet_classification_state_dict_280_no_finetuned_optimizer.pth')\n",
    "            #model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "            #output_model_file = os.path.join(\"bert_tweet_classification_280_no_finetuned_model.bin\")\n",
    "\n",
    "        else:\n",
    "            print(f'model did not improve')\n",
    "        \n",
    "        logging.info(f'We completed epoch {epoch+1} with a training loss of {np.sum(training_loss)/iterations} \\\n",
    "        a training accuracy of {np.mean(training_accuracy):,.2%}, Validation \\\n",
    "        loss is {np.sum(val_loss)/iterations_val} and validation accuracy is {np.mean(val_accuracy):,.2%}')\n",
    "      \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1 out of 3\n",
      "current training loss is 0.031585946679115295 and accuracy is 75.00%\n",
      "current training loss is 0.029187101870775223 and accuracy is 62.50%\n",
      "current training loss is 0.02686399593949318 and accuracy is 75.00%\n",
      "current training loss is 0.025872591882944107 and accuracy is 75.00%\n",
      "current training loss is 0.024858733639121056 and accuracy is 75.00%\n",
      "current training loss is 0.023933369666337967 and accuracy is 79.17%\n",
      "For epoch 1 training loss is 0.023506755009293556,         training accuracy is 79.17%, Validation         loss is 0.3957637548446655 and validation accuracy is 82.72%\n",
      "saving the model with validation accuracy of 82.72% \n",
      "starting epoch 2 out of 3\n",
      "current training loss is 0.01754308119416237 and accuracy is 75.00%\n",
      "current training loss is 0.01743929274380207 and accuracy is 87.50%\n",
      "current training loss is 0.017386216670274734 and accuracy is 83.33%\n",
      "current training loss is 0.017206910997629166 and accuracy is 87.50%\n",
      "current training loss is 0.01754823327064514 and accuracy is 85.00%\n",
      "current training loss is 0.01725289598107338 and accuracy is 79.17%\n",
      "For epoch 2 training loss is 0.017239287495613098,         training accuracy is 79.17%, Validation         loss is 0.37647902965545654 and validation accuracy is 84.42%\n",
      "saving the model with validation accuracy of 84.42% \n",
      "starting epoch 3 out of 3\n",
      "current training loss is 0.014608885161578655 and accuracy is 75.00%\n",
      "current training loss is 0.013288277201354504 and accuracy is 87.50%\n",
      "current training loss is 0.013109307736158371 and accuracy is 91.67%\n",
      "current training loss is 0.013706445693969727 and accuracy is 93.75%\n",
      "current training loss is 0.01402234472334385 and accuracy is 85.00%\n",
      "current training loss is 0.013946272432804108 and accuracy is 83.33%\n",
      "For epoch 3 training loss is 0.01409879233688116,         training accuracy is 83.33%, Validation         loss is 0.41648656129837036 and validation accuracy is 82.85%\n",
      "model did not improve\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('bert_tweet_classification_state_dict_280_no_finetuned.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0                 Just happened a terrible car crash\n",
       "1   2  Heard about #earthquake is different cities, s...\n",
       "2   3  there is a forest fire at spot pond, geese are...\n",
       "3   9           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets = load_data(test, test = True)\n",
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Processing_predictions(object):\n",
    "    def __init__(self, text_column, id_column):\n",
    "        \n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "        self.id_column = id_column.tolist()\n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        tokenized_text = tokenizer.tokenize(self.text_column[index])\n",
    "        \n",
    "        if len(tokenized_text) > max_seq_length - 2:\n",
    "            tokenized_text = tokenized_text[0:(max_seq_length - 2)]\n",
    "\n",
    "        # We add the CLS token at the beginning of the tokenized sequence and the \n",
    "        # SEP token at the end.\n",
    "        tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "\n",
    "        # convert the inputs to ids (dict looking)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "\n",
    "        attention_masks = [1 if x>0 else 0 for x in input_ids] \n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(attention_masks) == max_seq_length\n",
    "        #print(ids_review)\n",
    "        ids = self.id_column[index]\n",
    "        #list_of_ids = torch.tensor(ids)\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        #list_of_labels = [torch.from_numpy(np.array(labels)).squeeze(0)]\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "         \n",
    "        return input_ids, attention_masks, ids\n",
    "    def __len__(self):\n",
    "        return len(self.text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# create a class to process the traininga and test data\n",
    "prediction_data = Data_Processing_predictions(test_tweets['text'], test_tweets['id'])\n",
    "\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict_pred = {'pred': DataLoader(prediction_data, \n",
    "                                            batch_size=batch_size, shuffle=True, num_workers=10),\n",
    "                   }\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101,  1996,  5469,  ...,     0,     0,     0],\n",
       "         [  101,  3531, 21357,  ...,     0,     0,     0],\n",
       "         [  101,  2034,  6869,  ...,     0,     0,     0],\n",
       "         [  101,  2792,  1015,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([3251,  142, 5422, 7014])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1 = next(iter(dataloaders_dict_pred.get('pred')))\n",
    "example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft1.load_state_dict(torch.load('../models/bert_tweet_classification_state_dict_280_no_finetuned.pth'))\n",
    "#os.path.join(\"../models/bert_tweet_classification_280_no_finetuned_model.bin\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_model(model = None):\n",
    "    prediction_data_frame_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (inputs, attention_mask, ids) in enumerate(dataloaders_dict_pred['pred']):\n",
    "                \n",
    "                inputs = inputs.to(device) \n",
    "                attention_mask = attention_mask.to(device)\n",
    "          \n",
    "              # feed the sequences to the model, specifying the attention mask\n",
    "                outputs = model(inputs, attention_mask=attention_mask)\n",
    "                \n",
    "              # feed the logits returned by the model to the softmax to classify the function\n",
    "                outputs = F.softmax(outputs[0],dim=1)\n",
    "                \n",
    "              # calculate the loss function, squeeze the labels so their shapes are compatible\n",
    "                #loss_eval_manual = criterion(outputs, label.squeeze(0))\n",
    "                \n",
    "                preds1 = torch.argmax(outputs,dim=1).cpu().data.numpy()\n",
    "                ids = ids.cpu().data.numpy()\n",
    "                \n",
    "                temp_data = pd.DataFrame(zip(ids,preds1), columns = ['id', 'target'])\n",
    "                prediction_data_frame_list.append(temp_data)                \n",
    "    \n",
    "    prediction_df = pd.concat(prediction_data_frame_list)\n",
    "    return prediction_df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions_model(model = model_ft1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feb 18 2020 12:38:24'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "date = datetime.datetime.now().strftime(\"%b %d %Y %H:%M:%S\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feb_18_2020_12:38:24'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = date.replace(\" \", \"_\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('../data/'+date+'submission_bert_fine_tuned.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
