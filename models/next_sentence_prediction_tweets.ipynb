{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of training data is 5,479 and test data is 625\n"
     ]
    }
   ],
   "source": [
    "# laod training and test data using a shuffle function\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "training_data = pd.read_csv('../data/fine_tune_training_dataset.csv')\n",
    "training_data = shuffle(training_data)\n",
    "test_data = pd.read_csv('../data/fine_tune_test_dataset.csv')\n",
    "test_data = shuffle(test_data)\n",
    "print(f'Lenght of training data is {len(training_data):,} and test data is {len(test_data):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>There has not been 1 real tear out of #Shelli ...</td>\n",
       "      <td>#bb17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>phew!</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Patience Jonathan</td>\n",
       "      <td>KS315</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>@rachelcaine</td>\n",
       "      <td>In...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>am boy @Crash</td>\n",
       "      <td>.fuck</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Full read</td>\n",
       "      <td>Here:</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>Good tips!</td>\n",
       "      <td>_????</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hiroshima survivors fight nuclear industry in ...</td>\n",
       "      <td>video</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>MH370: debris found on reunion island. ??</td>\n",
       "      <td>MH370</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>New item:</td>\n",
       "      <td>THATS</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence_1 sentence_2  label  len\n",
       "182  There has not been 1 real tear out of #Shelli ...      #bb17      0    5\n",
       "551  The Latest: More Homes Razed by Northern Calif...      phew!      1    5\n",
       "494                                  Patience Jonathan      KS315      1    5\n",
       "576                                       @rachelcaine      In...      1    5\n",
       "438                                      am boy @Crash      .fuck      1    5\n",
       "367                                          Full read      Here:      1    5\n",
       "543                                         Good tips!      _????      1    5\n",
       "22   Hiroshima survivors fight nuclear industry in ...      video      0    5\n",
       "559          MH370: debris found on reunion island. ??      MH370      1    5\n",
       "450                                          New item:      THATS      1    5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['len']=test_data['sentence_2'].apply(lambda x: len(x))\n",
    "test_data[test_data['len']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to define a function to go over the tokens returned by the BERT tokenizer. We will skip special \n",
    "# characters and follow the proportion of masked tokens,\n",
    "# \n",
    "# refer this file for the mask_token function https://github.com/google-research/bert/blob/master/create_pretraining_data.py\n",
    "\n",
    "def mask_tokens(inputs):\n",
    "    '''inputs: This the pair of sentences tokenized by the BERT tokenizer. This script returns \n",
    "    a the list of inputs and the masked labels. '''\n",
    "    \n",
    "    #copy the inputs\n",
    "    labels = inputs.copy()\n",
    "    \n",
    "    # create a list of the indexes we are going to modify with a masked label\n",
    "    index_to_replace = []\n",
    "    # create a lost with the labels we are going to predict\n",
    "    labels_filtered = []\n",
    "\n",
    "    # create index of observations to replace, we skip masking the CLS and SEP special tokens\n",
    "    for (index, element) in enumerate(labels):\n",
    "        if element=='[CLS]'or element== '[SEP]':\n",
    "            continue\n",
    "        else:\n",
    "            index_to_replace.append(index)\n",
    "            labels_filtered.append(element)\n",
    "        \n",
    "    # get a list of the indices we are masking (85 percent mask, 10 percent random, 10 percent no change))\n",
    "    # masked_indices= np.random.binomial(size=len(index_to_replace), n=2, p= 0.15)\n",
    "    masked_indices = np.random.choice([0, 1], size=len(index_to_replace), p=[.85, .15])\n",
    "    \n",
    "    # create a list to store the labels of the masked tokens\n",
    "    labels_masked_token = []\n",
    "    # the masked indices contains the indices that we are going to mask, we iterate to get the index position \n",
    "    # of the indices we need to replace\n",
    "    for index_position, index_bool in zip(index_to_replace, list(masked_indices)):\n",
    "        if index_bool==1:\n",
    "            labels_masked_token.append(index_position)\n",
    "        else:\n",
    "            continue \n",
    "    \n",
    "    # once we have that letter we will replace those tokens with the masked token\n",
    "    indices_replaced_mask_tokens = np.random.binomial(size=len(labels_masked_token), n=1, p= 0.8)\n",
    "    \n",
    "    # for the remaining 20 percent tokens half of the time we will replace them with a random token and the other\n",
    "    # half we leave the original token in place.\n",
    "    indices_replaced_mask_rand = np.random.binomial(size=len(labels_masked_token), n=1, p= 0.5) & ~indices_replaced_mask_tokens \n",
    "    \n",
    "    # We now use our copy of the tokends, convert that into the ids of the vocabulary and start the replacement\n",
    "    # process\n",
    "    labels=tokenizer.convert_tokens_to_ids(labels)\n",
    "    \n",
    "    # We first replace the tokens masked with the id of the special [MASK] token in our vocabulary which \n",
    "    # corresponds to 100\n",
    "    for boolean, true_index in zip(indices_replaced_mask_tokens, labels_masked_token):\n",
    "        if boolean==1:\n",
    "            labels[true_index]=100\n",
    "    \n",
    "    # For the tokens that need to be replaced with a random token in the range of the len of the vocabulary\n",
    "    for boolean, true_index in zip(indices_replaced_mask_rand, labels_masked_token):\n",
    "        if boolean==1:\n",
    "            labels[true_index]= np.random.randint(0,len(tokenizer.vocab), size=1)[0]\n",
    "    \n",
    "    # create a list of the position masked tokens\n",
    "    position_masked_tokens= []\n",
    "    \n",
    "    # we finally parse everything and add -1 to the tokens we did not mask.\n",
    "    for index,token in enumerate(labels):\n",
    "        if index in labels_masked_token:\n",
    "            position_masked_tokens.append(tokenizer.convert_tokens_to_ids(inputs[index]))\n",
    "        else:\n",
    "            position_masked_tokens.append(-1)\n",
    "    \n",
    "    return labels, position_masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our dataloader class\n",
    "# create class to load a review, split into sentences, check if the sentences are in line with max lenght\n",
    "# reference this post https://github.com/ceshine/pytorch-pretrained-BERT/blob/master/notebooks/Next%20Sentence%20Prediction.ipynb?source=post_page-----1dbfe6a66f1d----------------------\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForNextSentencePrediction\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_seq_length = 128\n",
    "\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, sentence_1, sentence_2, label):\n",
    "        self.sentence_1 = sentence_1.tolist()\n",
    "        self.sentence_2 = sentence_2.tolist()\n",
    "        self.label = label.tolist()\n",
    "        \n",
    "    # define the text column from the dataframe\n",
    "        assert isinstance(self.sentence_1, list), 'Argument of wrong type!'\n",
    "        assert isinstance(self.sentence_2, list), 'Argument of wrong type!'\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_1) \n",
    "\n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence_1_tokenized_text = tokenizer.tokenize(self.sentence_1[index])\n",
    "        sentence_2_tokenized_text = tokenizer.tokenize(self.sentence_2[index])\n",
    "        \n",
    "        while True:\n",
    "            total_length = len(sentence_1_tokenized_text) + len(sentence_2_tokenized_text)\n",
    "\n",
    "            if total_length <= (max_seq_length-3):\n",
    "                break\n",
    "            if len(sentence_1_tokenized_text) > len(sentence_2_tokenized_text):\n",
    "                sentence_1_tokenized_text.pop()\n",
    "            else:\n",
    "                sentence_2_tokenized_text.pop()\n",
    "\n",
    "#sentence_1_tokenized_tex_ids = tokenizer.convert_tokens_to_ids(sentence_1_tokenized_tex_ids)\n",
    "#sentence_2_tokenized_tex_ids = tokenizer.convert_tokens_to_ids(sentence_2_tokenized_tex_ids)\n",
    "\n",
    "        inputs = [\"[CLS]\"] + sentence_1_tokenized_text + [\"[SEP]\"]\n",
    "        inputs +=  sentence_2_tokenized_text + [\"[SEP]\"]\n",
    "        # \n",
    "        segment_ids = [0] * (len(sentence_1_tokenized_text)+2)\n",
    "        segment_ids += [1] * (len(sentence_2_tokenized_text) + 1)\n",
    "\n",
    "        # get the inputs id's (with the masked tokens) as well as the labels of the masked tokens\n",
    "        input_ids, masked_lm_labels = mask_tokens(inputs)\n",
    "\n",
    "         # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        padding_masked = [-1] * (max_seq_length - len(input_ids))\n",
    "\n",
    "        #add padding in case it is necessary\n",
    "        input_ids += padding\n",
    "        segment_ids += padding\n",
    "        attention_masks = [1 if x>0 else 0 for x in input_ids]\n",
    "        masked_lm_labels += padding_masked\n",
    "\n",
    "        # Checkt the lenghts of the sequences\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert inputs[0] == \"[CLS]\"\n",
    "        assert input_ids[0]== 101\n",
    "        assert len(attention_masks) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(masked_lm_labels) == max_seq_length\n",
    "\n",
    "        # convert to torch tensor\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        segment_ids = torch.tensor(segment_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "        masked_lm_labels = torch.tensor(masked_lm_labels)\n",
    "\n",
    "        # get sequence label\n",
    "        sequence_label = self.label[index]\n",
    "\n",
    "        return input_ids, segment_ids, attention_masks, masked_lm_labels, sequence_label\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "# reference for dataloaders https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "%time\n",
    "batch_size = 12\n",
    "\n",
    "# create a class to process the traininga and test data\n",
    "training_data = Data_Processing(training_data['sentence_1'],\n",
    "                                training_data['sentence_2'],\n",
    "                                training_data['label'])\n",
    "\n",
    "test_data =  Data_Processing(test_data['sentence_1'], \n",
    "                             test_data['sentence_2'], \n",
    "                             test_data['label'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 6.68 µs\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                'val':len(test_data)}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101, 26539,   100,  ...,     0,     0,     0],\n",
       "         [  101,  1030,  2032,  ...,     0,     0,     0],\n",
       "         [  101,  8902, 25311,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2066,  2339,  ...,     0,     0,     0],\n",
       "         [  101, 21318, 13149,  ...,     0,     0,     0],\n",
       "         [  101,   100, 24459,  ...,     0,     0,     0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([[  -1,   -1, 5488,  ...,   -1,   -1,   -1],\n",
       "         [  -1,   -1,   -1,  ...,   -1,   -1,   -1],\n",
       "         [  -1,   -1,   -1,  ...,   -1,   -1,   -1],\n",
       "         ...,\n",
       "         [  -1,   -1,   -1,  ...,   -1,   -1,   -1],\n",
       "         [  -1,   -1,   -1,  ...,   -1,   -1,   -1],\n",
       "         [  -1, 6016,   -1,  ...,   -1,   -1,   -1]]),\n",
       " tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = next(iter(dataloaders_dict.get('train')))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "config = BertConfig()\n",
    "from transformers import BertForPreTraining\n",
    "model = BertForPreTraining.from_pretrained(\n",
    "    \"bert-base-uncased\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "from torch.optim import lr_scheduler, AdamW\n",
    "#from transformers import WarmupLinearSchedule\n",
    "\n",
    "lrlast = .001\n",
    "#lrmain = .00001\n",
    "lrmain = 2e-5\n",
    "\n",
    "\n",
    "optim1 = optim.AdamW(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "   ])\n",
    "\n",
    "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim1\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "number_steps= 4\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=3):\n",
    "    best_eval_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        #zero the model gradients\n",
    "        model.zero_grad()\n",
    "        print(f'starting epoch {epoch+1} out of {num_epochs}')\n",
    "        training_loss = []\n",
    "        training_accuracy_next_sentence = []\n",
    "        training_accuracy_vocab = []\n",
    "        val_loss = []\n",
    "        val_accuracy_next_sentence = []\n",
    "        val_accuracy_vocab = []\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # track number of batches and number of iterations\n",
    "        counter = 0\n",
    "        iterations = 0\n",
    "        counter_val = 0\n",
    "        iterations_val = 0\n",
    "\n",
    "        # Iterate over data, feeding inputs, attention masks and labels\n",
    "        model.train()\n",
    "        for i, (inputs, segment_ids, attention_masks, masked_lm_labels, sequence_label) in enumerate(dataloaders_dict['train']):\n",
    "            # add a counter that will register how many examples we have fed to thee model\n",
    "            counter += batch_size\n",
    "            iterations += 1\n",
    "            # move the sequences, labels and masks to the GPU\n",
    "            inputs = inputs.to(device) \n",
    "              #print(inputs)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            #print(segment_ids)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            #print(attention_mask)\n",
    "\n",
    "            masked_lm_labels = masked_lm_labels.to(device)\n",
    "            sequence_label = sequence_label.to(device)\n",
    "\n",
    "            # feed the sequences to the model, specifying the attention mask\n",
    "            outputs = model(inputs, token_type_ids = segment_ids, attention_mask= attention_masks, masked_lm_labels=masked_lm_labels,\n",
    "                         next_sentence_label=sequence_label)\n",
    "\n",
    "            # feed the logits returned by the model to the softmax to classify the function\n",
    "            prediction_scores = outputs[1]\n",
    "            seq_relationship_scores = outputs[2]\n",
    "\n",
    "            # TO DO Accuracy of model\n",
    "\n",
    "            #\n",
    "            loss_reg = outputs[0]/number_steps\n",
    "\n",
    "            #add the loss to the epoch loss\n",
    "            epoch_loss += loss_reg\n",
    "            training_loss.append(loss_reg)\n",
    "\n",
    "            outputs[0].backward()\n",
    "\n",
    "            # accumulate gradients and update every 4 batches\n",
    "            if (i+1) % number_steps == 0:\n",
    "                optimizer.step()                            # Now we can do an optimizer step\n",
    "                model.zero_grad()\n",
    "\n",
    "          # only present the information \n",
    "            if counter% 10 == 0:\n",
    "            # get the predictions and the true labels out of the GPU\n",
    "                preds_next_sentence = torch.argmax(seq_relationship_scores,dim=1).cpu().data.numpy()\n",
    "                true_next_sentence = np.array(sequence_label.squeeze(0).cpu().data.numpy())\n",
    "\n",
    "            #print('here', true_next_sentence.shape, preds_next_sentence.shape)\n",
    "\n",
    "                accuracy_next_sentence = accuracy_score(true_next_sentence,preds_next_sentence)\n",
    "                training_accuracy_next_sentence.append(accuracy_next_sentence)\n",
    "\n",
    "\n",
    "            # repeat with softmax for vocabulary\n",
    "            #masked_predictions = F.softmax(prediction_scores,dim=1)\n",
    "            #masked_predictions = torch.argmax(masked_predictions, 1).cpu().data.numpy()\n",
    "\n",
    "            #true_vocab = np.array(masked_lm_labels.cpu().data.numpy())\n",
    "            #print('now here', masked_predictions.shape, true_vocab.shape)\n",
    "            #print(masked_predictions)\n",
    "            #accuracy_vocab = accuracy_score(masked_predictions,true_vocab)\n",
    "            #accuracy_vocab = sum(masked_predictions == true_vocab.squeeze(1))/len(true_vocab)\n",
    "                print(f'current training loss is {np.sum(training_loss)/iterations} \\\n",
    "                and next sent accuracy is {np.mean(training_accuracy_next_sentence):,.2%}')\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            counter_val = 0\n",
    "            iterations_val = 0\n",
    "            \n",
    "            for i, (inputs, segment_ids, attention_masks, masked_lm_labels, sequence_label) in enumerate(dataloaders_dict['val']):\n",
    "                counter_val += batch_size\n",
    "                iterations_val += 1\n",
    "\n",
    "                inputs = inputs.to(device) \n",
    "                #print(inputs)\n",
    "                segment_ids = segment_ids.to(device)\n",
    "                #print(segment_ids)\n",
    "                attention_masks = attention_masks.to(device)\n",
    "                #print(attention_mask)\n",
    "\n",
    "                masked_lm_labels = masked_lm_labels.to(device)\n",
    "                sequence_label = sequence_label.to(device)\n",
    "\n",
    "                # feed the sequences to the model, specifying the attention mask\n",
    "                outputs = model(inputs, token_type_ids = segment_ids, attention_mask= attention_masks, \n",
    "                                masked_lm_labels = masked_lm_labels,\n",
    "                                next_sentence_label = sequence_label)\n",
    "\n",
    "                prediction_scores = outputs[1]\n",
    "                seq_relationship_scores = outputs[2]\n",
    "\n",
    "                # calculate the loss function, squeeze the labels so their shapes are compatible\n",
    "                loss_eval = outputs[0] \n",
    "                val_loss.append(loss_eval)\n",
    "\n",
    "                # get the predictions and the true labels out of the GPU for validation\n",
    "                preds_next_sentence = torch.argmax(seq_relationship_scores,dim=1).cpu().data.numpy()\n",
    "                print(preds_next_sentence)\n",
    "                true_next_sentence = np.array(sequence_label.squeeze(0).cpu().data.numpy())\n",
    "                accuracy_next_sentence = accuracy_score(true_next_sentence,preds_next_sentence)\n",
    "\n",
    "                val_accuracy_next_sentence.append(accuracy_next_sentence)\n",
    "\n",
    "                if counter_val % 100 == 0:\n",
    "\n",
    "                    print(f'current val loss is {np.sum(val_loss)/iterations_val} \\\n",
    "                    and next sent accuracy val is {np.mean(val_accuracy_next_sentence):,.2%}')\n",
    "              \n",
    "        print(f'For epoch {epoch+1} training loss is {np.sum(training_loss)/iterations}, \\\n",
    "        training accuracy is {np.mean(training_accuracy_next_sentence):,.2%}, Validation loss is {np.sum(val_loss)/iterations_val} \\\n",
    "        and validation accuracy is {np.mean(val_accuracy_next_sentence):,.2%}')\n",
    "\n",
    "        eval_acc = np.mean(val_accuracy_next_sentence)\n",
    "        if eval_acc >= best_eval_acc:\n",
    "            best_eval_acc = eval_acc\n",
    "            print(f'saving the model with validation accuracy of {eval_acc:,.2%} ')\n",
    "            torch.save(model.state_dict(), '../data/bert_tweet_language_finetune_model.pth')\n",
    "            #torch.save(model.state_dict(), 'bert_imdb_pretrain.pth')\n",
    "            #torch.save(optimizer_ft.state_dict(), 'bert_imdb_optimiser_pretrain.pth')\n",
    "            print('now saving as recommended by huggingface')\n",
    "            #model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "            #output_model_file = os.path.join(\"bert_pretrained_model.bin\")\n",
    "            #torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "            # model loading weights\n",
    "\n",
    "            model.load_state_dict(torch.load('../data/bert_tweet_language_finetune_model.pth'))\n",
    "            #optimizer.load_state_dict(torch.load('bert_imdb_optimiser_pretrain.pth'))\n",
    "\n",
    "        else:\n",
    "            print(f'model did not improve')\n",
    "\n",
    "      # load     \n",
    "          #if epoch_loss<previous_loss:\n",
    "          #  print(f'saving the model with epoch loss {epoch_loss} of and accuracy of {np.mean(accuracy_):,.2%}')\n",
    "          #  torch.save(model.state_dict(), 'bert_imdb.pth')\n",
    "          #  torch.save(optimiser.state_dict(), 'bert_imdb_optimiser.pth')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1 out of 3\n",
      "current training loss is 3.5915982723236084                 and next sent accuracy is 66.67%\n",
      "current training loss is 3.1834590435028076                 and next sent accuracy is 58.33%\n",
      "current training loss is 2.8794779777526855                 and next sent accuracy is 58.33%\n",
      "current training loss is 2.653743267059326                 and next sent accuracy is 54.17%\n",
      "current training loss is 2.490097999572754                 and next sent accuracy is 51.67%\n",
      "current training loss is 2.3602707386016846                 and next sent accuracy is 52.78%\n",
      "current training loss is 2.2567176818847656                 and next sent accuracy is 52.38%\n",
      "current training loss is 2.165539264678955                 and next sent accuracy is 55.21%\n",
      "current training loss is 2.0949466228485107                 and next sent accuracy is 56.48%\n",
      "current training loss is 2.0257456302642822                 and next sent accuracy is 56.67%\n",
      "current training loss is 1.9856339693069458                 and next sent accuracy is 56.82%\n",
      "current training loss is 1.9362562894821167                 and next sent accuracy is 59.03%\n",
      "current training loss is 1.896268367767334                 and next sent accuracy is 57.69%\n",
      "current training loss is 1.855625033378601                 and next sent accuracy is 57.74%\n",
      "current training loss is 1.8246122598648071                 and next sent accuracy is 56.67%\n",
      "current training loss is 1.7992452383041382                 and next sent accuracy is 56.77%\n",
      "current training loss is 1.7777563333511353                 and next sent accuracy is 56.86%\n",
      "current training loss is 1.7474260330200195                 and next sent accuracy is 56.94%\n",
      "current training loss is 1.7255513668060303                 and next sent accuracy is 56.14%\n",
      "current training loss is 1.6995933055877686                 and next sent accuracy is 56.67%\n",
      "current training loss is 1.6803048849105835                 and next sent accuracy is 57.14%\n",
      "current training loss is 1.6586530208587646                 and next sent accuracy is 57.20%\n",
      "current training loss is 1.6417820453643799                 and next sent accuracy is 56.88%\n",
      "current training loss is 1.6229157447814941                 and next sent accuracy is 56.94%\n",
      "current training loss is 1.6090948581695557                 and next sent accuracy is 58.00%\n",
      "current training loss is 1.5969388484954834                 and next sent accuracy is 57.69%\n",
      "current training loss is 1.5859872102737427                 and next sent accuracy is 58.02%\n",
      "current training loss is 1.571815848350525                 and next sent accuracy is 58.33%\n",
      "current training loss is 1.5629938840866089                 and next sent accuracy is 58.05%\n",
      "current training loss is 1.5482065677642822                 and next sent accuracy is 58.06%\n",
      "current training loss is 1.5372059345245361                 and next sent accuracy is 57.80%\n",
      "current training loss is 1.5238574743270874                 and next sent accuracy is 57.29%\n",
      "current training loss is 1.512982726097107                 and next sent accuracy is 57.58%\n",
      "current training loss is 1.503078579902649                 and next sent accuracy is 57.84%\n",
      "current training loss is 1.4928616285324097                 and next sent accuracy is 57.62%\n",
      "current training loss is 1.4852583408355713                 and next sent accuracy is 57.64%\n",
      "current training loss is 1.476391315460205                 and next sent accuracy is 57.88%\n",
      "current training loss is 1.4681724309921265                 and next sent accuracy is 57.89%\n",
      "current training loss is 1.4587700366973877                 and next sent accuracy is 58.33%\n",
      "current training loss is 1.4517141580581665                 and next sent accuracy is 58.13%\n",
      "current training loss is 1.4442304372787476                 and next sent accuracy is 57.93%\n",
      "current training loss is 1.439429759979248                 and next sent accuracy is 57.74%\n",
      "current training loss is 1.4339218139648438                 and next sent accuracy is 57.75%\n",
      "current training loss is 1.427807331085205                 and next sent accuracy is 58.14%\n",
      "current training loss is 1.4193252325057983                 and next sent accuracy is 58.33%\n",
      "current training loss is 1.4131830930709839                 and next sent accuracy is 58.51%\n",
      "current training loss is 1.4082695245742798                 and next sent accuracy is 58.69%\n",
      "current training loss is 1.4028453826904297                 and next sent accuracy is 59.03%\n",
      "current training loss is 1.3970909118652344                 and next sent accuracy is 59.18%\n",
      "current training loss is 1.3930277824401855                 and next sent accuracy is 59.00%\n",
      "current training loss is 1.3885447978973389                 and next sent accuracy is 59.80%\n",
      "current training loss is 1.3812905550003052                 and next sent accuracy is 59.78%\n",
      "current training loss is 1.3747748136520386                 and next sent accuracy is 60.06%\n",
      "current training loss is 1.3696657419204712                 and next sent accuracy is 60.03%\n",
      "current training loss is 1.365755558013916                 and next sent accuracy is 59.85%\n",
      "current training loss is 1.360427975654602                 and next sent accuracy is 59.97%\n",
      "current training loss is 1.3559000492095947                 and next sent accuracy is 60.09%\n",
      "current training loss is 1.351109504699707                 and next sent accuracy is 60.78%\n",
      "current training loss is 1.3461374044418335                 and next sent accuracy is 60.73%\n",
      "current training loss is 1.3418951034545898                 and next sent accuracy is 61.11%\n",
      "current training loss is 1.3389636278152466                 and next sent accuracy is 61.34%\n",
      "current training loss is 1.333778977394104                 and next sent accuracy is 61.69%\n",
      "current training loss is 1.330714464187622                 and next sent accuracy is 61.64%\n",
      "current training loss is 1.326595425605774                 and next sent accuracy is 61.98%\n",
      "current training loss is 1.3237333297729492                 and next sent accuracy is 61.92%\n",
      "current training loss is 1.3211859464645386                 and next sent accuracy is 61.99%\n",
      "current training loss is 1.3170756101608276                 and next sent accuracy is 62.19%\n",
      "current training loss is 1.3134018182754517                 and next sent accuracy is 62.13%\n",
      "current training loss is 1.3107295036315918                 and next sent accuracy is 62.08%\n",
      "current training loss is 1.3072595596313477                 and next sent accuracy is 62.14%\n",
      "current training loss is 1.3051352500915527                 and next sent accuracy is 61.85%\n",
      "current training loss is 1.3026924133300781                 and next sent accuracy is 62.15%\n",
      "current training loss is 1.300734519958496                 and next sent accuracy is 62.44%\n",
      "current training loss is 1.2973241806030273                 and next sent accuracy is 62.61%\n",
      "current training loss is 1.2940210103988647                 and next sent accuracy is 62.33%\n",
      "current training loss is 1.290286898612976                 and next sent accuracy is 62.50%\n",
      "current training loss is 1.2870320081710815                 and next sent accuracy is 62.45%\n",
      "current training loss is 1.282367467880249                 and next sent accuracy is 62.29%\n",
      "current training loss is 1.279098391532898                 and next sent accuracy is 62.45%\n",
      "current training loss is 1.2759586572647095                 and next sent accuracy is 62.40%\n",
      "current training loss is 1.2726715803146362                 and next sent accuracy is 62.55%\n",
      "current training loss is 1.2709606885910034                 and next sent accuracy is 62.60%\n",
      "current training loss is 1.268390417098999                 and next sent accuracy is 62.85%\n",
      "current training loss is 1.2674968242645264                 and next sent accuracy is 62.80%\n",
      "current training loss is 1.264988899230957                 and next sent accuracy is 62.75%\n",
      "current training loss is 1.2633546590805054                 and next sent accuracy is 62.60%\n",
      "current training loss is 1.2595605850219727                 and next sent accuracy is 62.07%\n",
      "current training loss is 1.257753610610962                 and next sent accuracy is 61.93%\n",
      "current training loss is 1.2584627866744995                 and next sent accuracy is 62.17%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current training loss is 1.2576628923416138                 and next sent accuracy is 62.31%\n",
      "current training loss is 1.2552610635757446                 and next sent accuracy is 62.64%\n",
      "[1 0 1 1 0 0 0 0 1 1 0 1]\n",
      "[1 0 0 0 1 0 0 1 0 1 0 1]\n",
      "[0 1 0 1 1 0 0 0 1 1 1 1]\n",
      "[1 0 1 0 0 0 1 1 0 0 0 0]\n",
      "[1 0 1 0 0 1 0 1 0 1 1 1]\n",
      "[1 1 0 0 1 0 1 1 1 1 0 1]\n",
      "[1 1 1 1 1 1 0 0 1 0 1 1]\n",
      "[0 1 1 0 1 1 1 0 0 1 0 1]\n",
      "[1 0 1 1 0 1 1 1 1 0 1 1]\n",
      "[0 0 1 1 1 1 1 0 1 0 0 0]\n",
      "[0 1 1 0 1 1 0 0 1 1 1 1]\n",
      "[0 1 0 0 0 1 1 0 0 0 0 1]\n",
      "[1 0 0 1 1 1 0 1 1 0 0 1]\n",
      "[1 1 0 1 0 0 0 1 1 0 1 0]\n",
      "[1 1 1 1 1 0 0 0 1 1 0 0]\n",
      "[1 0 1 1 0 1 0 1 1 1 1 1]\n",
      "[1 1 1 1 1 0 1 1 0 1 0 1]\n",
      "[1 1 1 1 0 1 0 1 1 0 1 1]\n",
      "[0 0 0 0 1 1 1 0 0 0 1 1]\n",
      "[1 0 1 1 1 1 0 1 1 0 0 0]\n",
      "[1 1 1 0 1 1 1 0 1 1 0 1]\n",
      "[0 0 0 1 0 0 1 0 0 1 0 1]\n",
      "[0 1 1 0 1 0 0 0 1 0 1 1]\n",
      "[1 1 1 1 0 1 0 0 1 0 0 0]\n",
      "[1 0 0 1 0 0 1 1 1 0 1 0]\n",
      "current val loss is 4.2206268310546875                     and next sent accuracy val is 71.67%\n",
      "[1 0 0 0 0 1 1 1 1 1 1 1]\n",
      "[0 0 1 1 1 1 1 0 1 1 1 1]\n",
      "[0 0 0 1 0 1 1 1 1 1 1 1]\n",
      "[1 1 0 1 1 1 1 1 1 1 0 0]\n",
      "[0 1 0 0 0 1 1 1 1 1 1 0]\n",
      "[1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 1 1 0 0 0 1 1 0 1]\n",
      "[0 0 1 1 0 0 0 1 0 1 1 1]\n",
      "[1 0 0 0 0 1 0 1 0 0 0 1]\n",
      "[0 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[0 1 0 1 0 0 1 0 1 1 1 1]\n",
      "[1 1 1 1 1 0 0 1 1 0 1 0]\n",
      "[0 0 1 0 0 0 1 1 1 0 1 1]\n",
      "[1 0 1 1 0 0 1 0 0 0 1 1]\n",
      "[1 1 0 0 0 1 0 1 0 1 1 1]\n",
      "[0 1 1 1 1 0 1 1 1 1 0 1]\n",
      "[1 1 1 0 1 0 0 1 1 1 1 1]\n",
      "[0 1 0 0 0 0 1 1 1 1 1 0]\n",
      "[0 0 0 1 1 1 0 1 0 1 1 1]\n",
      "[1 1 1 1 1 0 0 1 1 1 1 1]\n",
      "[0 1 1 1 1 1 0 0 1 1 1 1]\n",
      "[1 1 0 0 0 1 0 0 1 0 0 1]\n",
      "[1 0 1 1 1 0 1 1 1 0 1 1]\n",
      "[1 0 1 1 0 1 0 1 1 1 1 1]\n",
      "[1 1 0 1 0 1 1 1 0 1 1 0]\n",
      "current val loss is 4.2092790603637695                     and next sent accuracy val is 68.00%\n",
      "[1 0 1 1 1 1 1 1 0 1 1 1]\n",
      "[0 1 0 1 0 1 1 0 1 1 1 0]\n",
      "For epoch 1 training loss is 1.2545077800750732,         training accuracy is 62.64%, Validation loss is 4.203601360321045         and validation accuracy is 67.95%\n",
      "saving the model with validation accuracy of 67.95% \n",
      "now saving as recommended by huggingface\n",
      "starting epoch 2 out of 3\n",
      "current training loss is 1.1124753952026367                 and next sent accuracy is 66.67%\n",
      "current training loss is 1.0647872686386108                 and next sent accuracy is 79.17%\n",
      "current training loss is 1.1080255508422852                 and next sent accuracy is 80.56%\n",
      "current training loss is 1.1124368906021118                 and next sent accuracy is 79.17%\n",
      "current training loss is 1.1317516565322876                 and next sent accuracy is 75.00%\n",
      "current training loss is 1.117979884147644                 and next sent accuracy is 72.22%\n",
      "current training loss is 1.1069836616516113                 and next sent accuracy is 72.62%\n",
      "current training loss is 1.0940474271774292                 and next sent accuracy is 69.79%\n",
      "current training loss is 1.0995999574661255                 and next sent accuracy is 67.59%\n",
      "current training loss is 1.101505160331726                 and next sent accuracy is 64.17%\n",
      "current training loss is 1.0976134538650513                 and next sent accuracy is 63.64%\n",
      "current training loss is 1.098960041999817                 and next sent accuracy is 62.50%\n",
      "current training loss is 1.0963242053985596                 and next sent accuracy is 63.46%\n",
      "current training loss is 1.1010267734527588                 and next sent accuracy is 62.50%\n",
      "current training loss is 1.1043345928192139                 and next sent accuracy is 63.89%\n",
      "current training loss is 1.101071834564209                 and next sent accuracy is 64.06%\n",
      "current training loss is 1.0940191745758057                 and next sent accuracy is 64.22%\n",
      "current training loss is 1.0967899560928345                 and next sent accuracy is 64.81%\n",
      "current training loss is 1.099747657775879                 and next sent accuracy is 64.04%\n",
      "current training loss is 1.09766685962677                 and next sent accuracy is 63.75%\n",
      "current training loss is 1.099960446357727                 and next sent accuracy is 63.89%\n",
      "current training loss is 1.097967267036438                 and next sent accuracy is 64.39%\n",
      "current training loss is 1.094835638999939                 and next sent accuracy is 65.58%\n",
      "current training loss is 1.0901966094970703                 and next sent accuracy is 65.97%\n",
      "current training loss is 1.0849062204360962                 and next sent accuracy is 67.33%\n",
      "current training loss is 1.0890151262283325                 and next sent accuracy is 67.63%\n",
      "current training loss is 1.0915817022323608                 and next sent accuracy is 67.90%\n",
      "current training loss is 1.084959626197815                 and next sent accuracy is 68.15%\n",
      "current training loss is 1.084364891052246                 and next sent accuracy is 67.82%\n",
      "current training loss is 1.0854040384292603                 and next sent accuracy is 68.33%\n",
      "current training loss is 1.088619589805603                 and next sent accuracy is 68.55%\n",
      "current training loss is 1.0881885290145874                 and next sent accuracy is 68.49%\n",
      "current training loss is 1.086133599281311                 and next sent accuracy is 68.43%\n",
      "current training loss is 1.0812954902648926                 and next sent accuracy is 67.89%\n",
      "current training loss is 1.0819051265716553                 and next sent accuracy is 68.33%\n",
      "current training loss is 1.0818982124328613                 and next sent accuracy is 68.52%\n",
      "current training loss is 1.0795224905014038                 and next sent accuracy is 68.47%\n",
      "current training loss is 1.080135464668274                 and next sent accuracy is 68.64%\n",
      "current training loss is 1.0806005001068115                 and next sent accuracy is 68.59%\n",
      "current training loss is 1.0792701244354248                 and next sent accuracy is 68.54%\n",
      "current training loss is 1.0806254148483276                 and next sent accuracy is 68.29%\n",
      "current training loss is 1.076033115386963                 and next sent accuracy is 68.45%\n",
      "current training loss is 1.077926754951477                 and next sent accuracy is 68.41%\n",
      "current training loss is 1.0792181491851807                 and next sent accuracy is 68.37%\n",
      "current training loss is 1.076171875                 and next sent accuracy is 67.78%\n",
      "current training loss is 1.0747311115264893                 and next sent accuracy is 67.57%\n",
      "current training loss is 1.0741454362869263                 and next sent accuracy is 67.38%\n",
      "current training loss is 1.0743529796600342                 and next sent accuracy is 67.53%\n",
      "current training loss is 1.07349693775177                 and next sent accuracy is 67.52%\n",
      "current training loss is 1.0740166902542114                 and next sent accuracy is 67.83%\n",
      "current training loss is 1.0715734958648682                 and next sent accuracy is 67.65%\n",
      "current training loss is 1.0752030611038208                 and next sent accuracy is 67.47%\n",
      "current training loss is 1.0728576183319092                 and next sent accuracy is 67.61%\n",
      "current training loss is 1.0729010105133057                 and next sent accuracy is 67.59%\n",
      "current training loss is 1.0740654468536377                 and next sent accuracy is 67.73%\n",
      "current training loss is 1.0738940238952637                 and next sent accuracy is 67.56%\n",
      "current training loss is 1.0735807418823242                 and next sent accuracy is 67.54%\n",
      "current training loss is 1.0722150802612305                 and next sent accuracy is 67.67%\n",
      "current training loss is 1.0714237689971924                 and next sent accuracy is 67.80%\n",
      "current training loss is 1.0705270767211914                 and next sent accuracy is 67.78%\n",
      "current training loss is 1.0704501867294312                 and next sent accuracy is 67.49%\n",
      "current training loss is 1.0693858861923218                 and next sent accuracy is 67.47%\n",
      "current training loss is 1.067220687866211                 and next sent accuracy is 67.46%\n",
      "current training loss is 1.0662424564361572                 and next sent accuracy is 67.71%\n",
      "current training loss is 1.0656317472457886                 and next sent accuracy is 67.82%\n",
      "current training loss is 1.065636157989502                 and next sent accuracy is 67.80%\n",
      "current training loss is 1.0656850337982178                 and next sent accuracy is 67.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current training loss is 1.0658082962036133                 and next sent accuracy is 67.77%\n",
      "current training loss is 1.066374659538269                 and next sent accuracy is 67.75%\n",
      "current training loss is 1.0642855167388916                 and next sent accuracy is 67.50%\n",
      "current training loss is 1.0633811950683594                 and next sent accuracy is 67.72%\n",
      "current training loss is 1.0645768642425537                 and next sent accuracy is 67.48%\n",
      "current training loss is 1.0649805068969727                 and next sent accuracy is 67.47%\n",
      "current training loss is 1.0652960538864136                 and next sent accuracy is 67.79%\n",
      "current training loss is 1.0636357069015503                 and next sent accuracy is 67.67%\n",
      "current training loss is 1.0622297525405884                 and next sent accuracy is 67.76%\n",
      "current training loss is 1.062690019607544                 and next sent accuracy is 67.64%\n",
      "current training loss is 1.0618051290512085                 and next sent accuracy is 67.74%\n",
      "current training loss is 1.0625343322753906                 and next sent accuracy is 67.83%\n",
      "current training loss is 1.061541199684143                 and next sent accuracy is 67.50%\n",
      "current training loss is 1.0615304708480835                 and next sent accuracy is 67.59%\n",
      "current training loss is 1.0615708827972412                 and next sent accuracy is 67.68%\n",
      "current training loss is 1.060500144958496                 and next sent accuracy is 67.87%\n",
      "current training loss is 1.0588691234588623                 and next sent accuracy is 67.86%\n",
      "current training loss is 1.0575824975967407                 and next sent accuracy is 67.75%\n",
      "current training loss is 1.0572932958602905                 and next sent accuracy is 67.93%\n",
      "current training loss is 1.0572900772094727                 and next sent accuracy is 68.01%\n",
      "current training loss is 1.057227611541748                 and next sent accuracy is 68.09%\n",
      "current training loss is 1.0578253269195557                 and next sent accuracy is 68.26%\n",
      "current training loss is 1.0581886768341064                 and next sent accuracy is 68.52%\n",
      "current training loss is 1.0582869052886963                 and next sent accuracy is 68.50%\n",
      "[1 1 1 0 1 0 1 1 0 1 0 1]\n",
      "[1 1 1 0 0 1 0 0 0 0 1 0]\n",
      "[1 1 1 1 1 0 0 1 0 1 0 1]\n",
      "[1 0 1 1 0 0 1 0 1 1 1 0]\n",
      "[1 1 0 1 1 0 0 1 0 0 1 1]\n",
      "[0 0 1 0 1 1 1 0 1 1 0 1]\n",
      "[1 1 1 1 0 0 1 1 0 1 0 1]\n",
      "[1 1 1 1 0 0 1 0 0 1 1 0]\n",
      "[1 0 1 0 1 0 1 0 1 0 0 0]\n",
      "[1 1 0 0 1 1 0 1 1 1 0 0]\n",
      "[1 0 0 1 1 0 0 1 0 1 1 1]\n",
      "[1 0 1 1 1 1 1 1 0 0 0 1]\n",
      "[1 1 0 1 1 1 1 0 0 1 1 1]\n",
      "[0 0 1 0 1 0 1 1 1 1 1 0]\n",
      "[1 0 1 1 0 0 0 0 1 1 1 1]\n",
      "[1 0 1 1 0 1 1 0 0 0 1 0]\n",
      "[0 1 1 1 1 1 0 0 0 1 1 1]\n",
      "[1 1 0 0 1 1 1 0 0 1 0 1]\n",
      "[1 1 1 1 0 0 0 1 1 0 1 0]\n",
      "[0 1 1 0 0 0 1 1 1 1 1 1]\n",
      "[0 0 1 0 1 1 1 0 0 0 1 1]\n",
      "[1 0 0 1 0 1 1 0 1 0 0 0]\n",
      "[1 0 1 1 0 1 0 1 0 0 1 0]\n",
      "[0 0 0 0 1 1 0 1 1 1 1 0]\n",
      "[1 0 1 1 1 0 0 1 1 0 0 1]\n",
      "current val loss is 3.8772006034851074                     and next sent accuracy val is 69.00%\n",
      "[1 1 1 0 1 1 0 1 1 1 0 0]\n",
      "[1 1 0 0 0 1 1 0 0 0 0 1]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "[0 1 0 0 0 1 0 1 0 0 0 0]\n",
      "[0 1 0 1 1 1 0 1 1 0 1 1]\n",
      "[0 1 0 1 0 0 1 0 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 0 0 0 1]\n",
      "[1 1 1 1 1 1 1 0 0 1 0 0]\n",
      "[1 1 1 1 0 0 0 1 1 1 1 1]\n",
      "[0 0 1 1 1 0 0 1 1 1 0 0]\n",
      "[0 0 0 1 0 0 1 1 0 1 0 1]\n",
      "[0 0 0 1 0 1 0 1 1 1 0 1]\n",
      "[0 1 0 0 1 0 0 0 0 0 0 1]\n",
      "[1 1 1 0 1 1 1 1 0 1 0 1]\n",
      "[0 1 1 1 0 0 1 0 1 1 0 0]\n",
      "[0 0 0 1 0 1 1 0 1 1 1 0]\n",
      "[1 0 0 0 0 1 0 1 1 1 1 0]\n",
      "[0 1 0 1 0 1 1 0 0 0 1 0]\n",
      "[1 0 0 1 1 1 1 1 1 1 0 1]\n",
      "[0 0 1 1 1 1 0 1 0 1 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 0 0 1]\n",
      "[1 1 0 0 1 0 1 1 0 0 1 0]\n",
      "[1 1 0 0 1 1 1 0 0 0 1 0]\n",
      "[1 0 0 0 0 1 1 0 1 1 0 1]\n",
      "[1 1 1 1 0 0 1 0 1 1 0 0]\n",
      "current val loss is 3.8139026165008545                     and next sent accuracy val is 70.17%\n",
      "[1 1 1 0 0 1 0 0 1 0 0 0]\n",
      "[0 1 0 1 1 0 0 1 1 1 0 0]\n",
      "For epoch 2 training loss is 1.0590134859085083,         training accuracy is 68.50%, Validation loss is 3.813673257827759         and validation accuracy is 69.55%\n",
      "saving the model with validation accuracy of 69.55% \n",
      "now saving as recommended by huggingface\n",
      "starting epoch 3 out of 3\n",
      "current training loss is 1.0381288528442383                 and next sent accuracy is 75.00%\n",
      "current training loss is 0.9810638427734375                 and next sent accuracy is 70.83%\n",
      "current training loss is 0.9888429641723633                 and next sent accuracy is 66.67%\n",
      "current training loss is 1.0004874467849731                 and next sent accuracy is 72.92%\n",
      "current training loss is 0.9958735704421997                 and next sent accuracy is 73.33%\n",
      "current training loss is 1.003960132598877                 and next sent accuracy is 76.39%\n",
      "current training loss is 1.0080965757369995                 and next sent accuracy is 75.00%\n",
      "current training loss is 1.0150035619735718                 and next sent accuracy is 76.04%\n",
      "current training loss is 1.0193970203399658                 and next sent accuracy is 75.93%\n",
      "current training loss is 1.019316554069519                 and next sent accuracy is 75.00%\n",
      "current training loss is 1.0231432914733887                 and next sent accuracy is 72.73%\n",
      "current training loss is 1.0134978294372559                 and next sent accuracy is 73.61%\n",
      "current training loss is 1.0036101341247559                 and next sent accuracy is 75.00%\n",
      "current training loss is 1.003110647201538                 and next sent accuracy is 74.40%\n",
      "current training loss is 1.0101507902145386                 and next sent accuracy is 73.33%\n",
      "current training loss is 1.0050309896469116                 and next sent accuracy is 73.96%\n",
      "current training loss is 1.0029728412628174                 and next sent accuracy is 74.51%\n",
      "current training loss is 0.9998202323913574                 and next sent accuracy is 75.46%\n",
      "current training loss is 0.9957133531570435                 and next sent accuracy is 74.56%\n",
      "current training loss is 0.9967086315155029                 and next sent accuracy is 75.00%\n",
      "current training loss is 0.996356725692749                 and next sent accuracy is 75.40%\n",
      "current training loss is 0.997403621673584                 and next sent accuracy is 76.14%\n",
      "current training loss is 0.9949290156364441                 and next sent accuracy is 76.81%\n",
      "current training loss is 0.9888847470283508                 and next sent accuracy is 76.74%\n",
      "current training loss is 0.9878409504890442                 and next sent accuracy is 75.33%\n",
      "current training loss is 0.9870595932006836                 and next sent accuracy is 75.96%\n",
      "current training loss is 0.9900885224342346                 and next sent accuracy is 75.62%\n",
      "current training loss is 0.9887253046035767                 and next sent accuracy is 75.60%\n",
      "current training loss is 0.9879551529884338                 and next sent accuracy is 76.15%\n",
      "current training loss is 0.9909146428108215                 and next sent accuracy is 75.56%\n",
      "current training loss is 0.9912780523300171                 and next sent accuracy is 75.81%\n",
      "current training loss is 0.9918322563171387                 and next sent accuracy is 75.26%\n",
      "current training loss is 0.9923416376113892                 and next sent accuracy is 75.25%\n",
      "current training loss is 0.9916219711303711                 and next sent accuracy is 75.98%\n",
      "current training loss is 0.9880403876304626                 and next sent accuracy is 76.19%\n",
      "current training loss is 0.9898155331611633                 and next sent accuracy is 76.16%\n",
      "current training loss is 0.990132212638855                 and next sent accuracy is 76.35%\n",
      "current training loss is 0.9895768165588379                 and next sent accuracy is 76.32%\n",
      "current training loss is 0.9910226464271545                 and next sent accuracy is 75.85%\n",
      "current training loss is 0.9900933504104614                 and next sent accuracy is 75.42%\n",
      "current training loss is 0.9899740219116211                 and next sent accuracy is 75.61%\n",
      "current training loss is 0.9894899129867554                 and next sent accuracy is 75.40%\n",
      "current training loss is 0.9842325448989868                 and next sent accuracy is 75.39%\n",
      "current training loss is 0.9825994968414307                 and next sent accuracy is 75.19%\n",
      "current training loss is 0.9830499887466431                 and next sent accuracy is 75.37%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current training loss is 0.9843931794166565                 and next sent accuracy is 75.18%\n",
      "current training loss is 0.9858165383338928                 and next sent accuracy is 74.82%\n",
      "current training loss is 0.9848302602767944                 and next sent accuracy is 74.31%\n",
      "current training loss is 0.9849754571914673                 and next sent accuracy is 74.83%\n",
      "current training loss is 0.9871969223022461                 and next sent accuracy is 74.67%\n",
      "current training loss is 0.9877802133560181                 and next sent accuracy is 74.84%\n",
      "current training loss is 0.9902846217155457                 and next sent accuracy is 74.68%\n",
      "current training loss is 0.9907011389732361                 and next sent accuracy is 74.21%\n",
      "current training loss is 0.993273138999939                 and next sent accuracy is 74.23%\n",
      "current training loss is 0.9951943159103394                 and next sent accuracy is 74.39%\n",
      "current training loss is 0.9945905208587646                 and next sent accuracy is 74.55%\n",
      "current training loss is 0.9948222041130066                 and next sent accuracy is 74.56%\n",
      "current training loss is 0.9965687394142151                 and next sent accuracy is 74.43%\n",
      "current training loss is 0.9954251646995544                 and next sent accuracy is 74.58%\n",
      "current training loss is 0.9946379661560059                 and next sent accuracy is 74.58%\n",
      "current training loss is 0.9951686263084412                 and next sent accuracy is 74.59%\n",
      "current training loss is 0.9952136278152466                 and next sent accuracy is 74.73%\n",
      "current training loss is 0.9944767355918884                 and next sent accuracy is 74.47%\n",
      "current training loss is 0.9940142631530762                 and next sent accuracy is 74.74%\n",
      "current training loss is 0.9950361251831055                 and next sent accuracy is 74.49%\n",
      "current training loss is 0.9957242608070374                 and next sent accuracy is 74.37%\n",
      "current training loss is 0.9973729252815247                 and next sent accuracy is 74.38%\n",
      "current training loss is 0.9973498582839966                 and next sent accuracy is 73.90%\n",
      "current training loss is 0.9972642660140991                 and next sent accuracy is 74.03%\n",
      "current training loss is 0.9978153705596924                 and next sent accuracy is 74.17%\n",
      "current training loss is 0.9979854822158813                 and next sent accuracy is 74.30%\n",
      "current training loss is 0.9976704120635986                 and next sent accuracy is 74.19%\n",
      "current training loss is 0.9965353012084961                 and next sent accuracy is 73.86%\n",
      "current training loss is 0.99592125415802                 and next sent accuracy is 73.99%\n",
      "current training loss is 0.994935929775238                 and next sent accuracy is 74.11%\n",
      "current training loss is 0.9952037334442139                 and next sent accuracy is 74.12%\n",
      "current training loss is 0.9953190088272095                 and next sent accuracy is 73.92%\n",
      "current training loss is 0.9949387907981873                 and next sent accuracy is 73.61%\n",
      "current training loss is 0.994113028049469                 and next sent accuracy is 73.73%\n",
      "current training loss is 0.9935464859008789                 and next sent accuracy is 73.85%\n",
      "current training loss is 0.992689311504364                 and next sent accuracy is 73.77%\n",
      "current training loss is 0.9940415024757385                 and next sent accuracy is 73.58%\n",
      "current training loss is 0.9931472539901733                 and next sent accuracy is 73.69%\n",
      "current training loss is 0.993010938167572                 and next sent accuracy is 73.71%\n",
      "current training loss is 0.9937331080436707                 and next sent accuracy is 73.73%\n",
      "current training loss is 0.9951847195625305                 and next sent accuracy is 73.84%\n",
      "current training loss is 0.9952892065048218                 and next sent accuracy is 73.95%\n",
      "current training loss is 0.9948291778564453                 and next sent accuracy is 74.05%\n",
      "current training loss is 0.9949144721031189                 and next sent accuracy is 74.16%\n",
      "current training loss is 0.9945160150527954                 and next sent accuracy is 74.17%\n",
      "current training loss is 0.9939597249031067                 and next sent accuracy is 74.45%\n",
      "[1 0 1 1 1 1 0 1 1 0 0 1]\n",
      "[1 0 0 1 1 0 0 1 1 0 1 1]\n",
      "[0 1 1 1 1 0 1 0 0 0 1 1]\n",
      "[0 1 1 1 0 1 0 0 1 1 0 1]\n",
      "[1 0 0 0 0 1 0 0 1 0 0 1]\n",
      "[0 0 1 0 0 1 1 0 0 1 0 0]\n",
      "[0 0 1 1 0 0 1 1 1 0 0 1]\n",
      "[0 1 0 1 0 0 0 0 1 0 0 1]\n",
      "[1 1 0 0 0 1 0 1 1 1 1 1]\n",
      "[0 1 1 0 0 0 1 1 0 0 1 1]\n",
      "[0 1 1 1 0 1 1 0 1 1 0 0]\n",
      "[1 0 0 0 1 1 0 0 1 0 1 0]\n",
      "[1 0 1 0 0 1 0 0 0 0 0 0]\n",
      "[1 1 1 1 0 1 1 1 1 0 0 0]\n",
      "[0 0 1 0 0 1 0 1 0 1 1 1]\n",
      "[1 0 1 0 1 1 1 0 1 1 1 0]\n",
      "[0 1 0 0 1 0 1 0 0 0 0 0]\n",
      "[1 1 0 1 1 1 0 1 1 0 1 1]\n",
      "[1 1 1 1 1 0 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 1 1 0 1 1 0 1]\n",
      "[1 0 1 0 1 0 0 0 1 0 1 0]\n",
      "[0 0 0 1 1 1 1 0 0 1 0 1]\n",
      "[0 0 1 0 0 0 1 1 0 0 1 1]\n",
      "[1 1 1 1 1 1 1 1 0 0 0 1]\n",
      "[1 1 0 0 1 0 1 0 1 0 0 0]\n",
      "current val loss is 3.819186210632324                     and next sent accuracy val is 69.00%\n",
      "[1 0 0 1 1 0 1 1 0 0 1 1]\n",
      "[0 1 1 0 0 0 1 1 0 0 0 1]\n",
      "[0 0 1 0 0 1 1 1 1 1 0 0]\n",
      "[1 1 0 0 1 0 0 1 1 0 1 1]\n",
      "[1 1 1 1 1 0 0 1 0 0 0 0]\n",
      "[0 0 0 1 1 1 1 1 0 0 0 1]\n",
      "[1 1 1 0 1 0 0 0 1 0 1 1]\n",
      "[1 1 0 1 1 0 0 0 1 1 1 0]\n",
      "[0 1 0 1 1 0 0 1 0 0 1 0]\n",
      "[1 1 1 0 0 1 0 1 1 0 1 1]\n",
      "[0 1 1 1 0 1 0 0 1 0 0 0]\n",
      "[1 1 0 1 1 1 0 1 1 0 1 1]\n",
      "[1 0 0 0 1 1 0 1 0 1 1 0]\n",
      "[1 0 1 1 0 1 1 0 1 0 1 1]\n",
      "[1 0 1 0 0 0 1 0 1 1 0 0]\n",
      "[0 0 1 0 1 1 1 1 0 0 1 0]\n",
      "[1 1 1 0 1 0 0 0 0 1 1 1]\n",
      "[1 0 0 0 0 1 0 0 1 1 0 1]\n",
      "[1 1 1 1 0 0 0 1 0 1 1 0]\n",
      "[1 1 1 0 1 0 1 0 1 1 1 1]\n",
      "[0 0 1 0 0 1 1 0 0 0 1 1]\n",
      "[0 0 0 0 0 1 1 0 0 0 0 1]\n",
      "[0 0 1 1 1 1 0 1 0 1 0 1]\n",
      "[0 1 0 1 1 1 1 0 1 1 1 1]\n",
      "[1 1 1 0 0 0 0 0 0 0 1 1]\n",
      "current val loss is 3.844716787338257                     and next sent accuracy val is 71.17%\n",
      "[1 0 1 0 1 0 1 1 1 1 1 1]\n",
      "[0 1 0 1 0 0 0 0 0 0 1 0]\n",
      "For epoch 3 training loss is 0.9938238859176636,         training accuracy is 74.45%, Validation loss is 3.8526837825775146         and validation accuracy is 71.31%\n",
      "saving the model with validation accuracy of 71.31% \n",
      "now saving as recommended by huggingface\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model_ft1 = train_model(model, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a442fd92cd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.post('https://textbelt.com/text', {\n",
    "  'phone': '4123131113',\n",
    "  'message': 'BERT for pretraining is done',\n",
    "  'key': 'textbelt',\n",
    "})\n",
    "print(resp.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
