{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory paths where we saved the tweets\n",
    "train = \"../data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for another implementation see https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_data(filename, test = False):\n",
    "    \n",
    "    data = pd.read_csv(filename)\n",
    "    if test:\n",
    "        data = data[['id','text']]\n",
    "        \n",
    "    else:\n",
    "        data = data[['text','target']]\n",
    "        \n",
    "        \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the unsupervised data into training and validation. We will use 80 percent of the dataset \n",
    "# for training and 20 percent for validation.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "supervised_train,  supervised_test = train_test_split(train_data, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now prepare the training and test data. We need to divide each data set in half. One half will be tokenized\n",
    "# to get sentences with the corresponding next sentence. The other half will have the same format except that \n",
    "# the sentences will come from random documents.\n",
    "\n",
    "supervised_train = np.array_split(supervised_train, 2)\n",
    "supervised_train_true = supervised_train[0]\n",
    "supervised_train_random = supervised_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat process with test data\n",
    "supervised_test = np.array_split(supervised_test, 2)\n",
    "supervised_test_true = supervised_test[0]\n",
    "supervised_test_random = supervised_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that takes a list of texts, tokenizes them, returns a datafrane and has an option to shuffle\n",
    "# the arrays\n",
    "# the next step is to define a function that takes a list of texts, in this case a dataframe, \n",
    "# pass them to the spacy tokenizer, get the sentences, filter sentences that are too small, and then return\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "spacy.prefer_gpu()\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "def sentence_tokenizer(text_sequences, randomize = False):\n",
    "    ''' text_sequence: A text document to be splitted into sentences'''\n",
    "    \n",
    "    assert isinstance(text_sequences, list)\n",
    "    \n",
    "    data_frame_list = []\n",
    "    \n",
    "    for index, text_sequence in enumerate(text_sequences):\n",
    "        text_sequence = re.sub('<br />', ' ', text_sequence)\n",
    "        \n",
    "        list_sentences = list(nlp.pipe([text_sequence], disable= ['tagger']))\n",
    "        \n",
    "        # create a list of sentences and make sure they are strings, else SpaCy throws and error\n",
    "        list_sentences = [list(x.sents) for x in list_sentences]\n",
    "        list_sentences = [[str(y) for y in x] for x in list_sentences]\n",
    "\n",
    "        # filter if the sentence has less than 4 words\n",
    "        list_sentences =  [[y for y in x if len(y)>=4] for x in list_sentences]\n",
    "\n",
    "        # we package all the sentences in lists of two using iter and zip two create a list of lists\n",
    "        list_sentences = [list(zip(*[iter(x)]*2)) for x in list_sentences]\n",
    "        list_sentences = [item for sublist in list_sentences for item in sublist]\n",
    "\n",
    "        # conver the list into a dataframe so we can split it in half.\n",
    "        data_original = pd.DataFrame(list_sentences, columns=['sentence_1','sentence_2'])\n",
    "        # add a label to indicate that the second sentence corresponds to the next sentence.\n",
    "        data_original['label'] = 0\n",
    "\n",
    "        # create a column of document id\n",
    "        data_original['document_id'] = index\n",
    "        data_original['document_id2'] = index\n",
    "        data_original['sentence_2a'] = data_original['sentence_2']\n",
    "        data_frame_list.append(data_original)\n",
    "    \n",
    "    data = pd.concat(data_frame_list, ignore_index=True)\n",
    "            \n",
    "    if randomize:\n",
    "        docs = data.groupby('document_id').count()\n",
    "        shuffle = docs.max().max()\n",
    "        data['sentence_2'] = np.roll(data['sentence_2'], shuffle)\n",
    "        data['label'] = 1\n",
    "              \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training data\n",
    "supervised_train_true_processed = sentence_tokenizer(supervised_train_true['text'].tolist())\n",
    "supervised_train_random_processed = sentence_tokenizer(supervised_train_random['text'].tolist(),\n",
    "                                                        randomize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test data\n",
    "supervised_test_true_processed = sentence_tokenizer(supervised_test_true['text'].tolist())\n",
    "supervised_test_random_processed = sentence_tokenizer(supervised_test_random['text'].tolist(),\n",
    "                                                        randomize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final train data\n",
    "supervised_processed_training = pd.concat([supervised_train_true_processed,\n",
    "                                   supervised_train_random_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final test data\n",
    "supervised_processed_test = pd.concat([supervised_test_true_processed,\n",
    "                                   supervised_test_random_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>document_id</th>\n",
       "      <th>document_id2</th>\n",
       "      <th>sentence_2a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@jasoncundy05 Chelsea need to hijack Man Utd d...</td>\n",
       "      <td>I don't have enough money for all the drugs an...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20 mill bargain Adam driving home in Oregon US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outdoor Siren Test 2</td>\n",
       "      <td>my fall bills</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>pm :</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>: The FGCU Siren will be tested at 2pm today.</td>\n",
       "      <td>HEM-712C Automatic Blood Pressure Monitor STAN...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Another message will be sent when the test is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He is justifying why this quarrel would one da...</td>\n",
       "      <td>http://t.co/rqKK15uhEY</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>https://t.co/z8Ij8KTkyk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Cowboys:</td>\n",
       "      <td>What a whirlwind of time it has been!</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>Wednesday's injury report: RB Lance Dunbar inj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Your brain is particularly vulnerable to traum...</td>\n",
       "      <td>20 mill bargain Adam driving home in Oregon US...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>http://t.co/KnBv2YtNWc @qz @TaraSwart @vivian_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IF ANYONE WERE TO HARM THE BOYS</td>\n",
       "      <td>pm :</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>THEY WOULD GET TAKEN DOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IMMEDIATELY NOT BY SECURITY</td>\n",
       "      <td>Another message will be sent when the test is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>BUT BY THE FANS REAL QUICK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#SanDiego #News Sinkhole Disrupts Downtown Tro...</td>\n",
       "      <td>https://t.co/z8Ij8KTkyk</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>: The incident happened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wed...</td>\n",
       "      <td>Wednesday's injury report: RB Lance Dunbar inj...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>http://t.co/RVMMuT3GvC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sentence_1  \\\n",
       "0  @jasoncundy05 Chelsea need to hijack Man Utd d...   \n",
       "1                               Outdoor Siren Test 2   \n",
       "2      : The FGCU Siren will be tested at 2pm today.   \n",
       "3  He is justifying why this quarrel would one da...   \n",
       "4                                          #Cowboys:   \n",
       "5  Your brain is particularly vulnerable to traum...   \n",
       "6                    IF ANYONE WERE TO HARM THE BOYS   \n",
       "7                        IMMEDIATELY NOT BY SECURITY   \n",
       "8  #SanDiego #News Sinkhole Disrupts Downtown Tro...   \n",
       "9                                             Wed...   \n",
       "\n",
       "                                          sentence_2  label  document_id  \\\n",
       "0  I don't have enough money for all the drugs an...      1            2   \n",
       "1                                      my fall bills      1            6   \n",
       "2  HEM-712C Automatic Blood Pressure Monitor STAN...      1            6   \n",
       "3                             http://t.co/rqKK15uhEY      1            7   \n",
       "4              What a whirlwind of time it has been!      1           11   \n",
       "5  20 mill bargain Adam driving home in Oregon US...      1           12   \n",
       "6                                               pm :      1           14   \n",
       "7  Another message will be sent when the test is ...      1           14   \n",
       "8                            https://t.co/z8Ij8KTkyk      1           15   \n",
       "9  Wednesday's injury report: RB Lance Dunbar inj...      1           15   \n",
       "\n",
       "   document_id2                                        sentence_2a  \n",
       "0             2  20 mill bargain Adam driving home in Oregon US...  \n",
       "1             6                                               pm :  \n",
       "2             6  Another message will be sent when the test is ...  \n",
       "3             7                            https://t.co/z8Ij8KTkyk  \n",
       "4            11  Wednesday's injury report: RB Lance Dunbar inj...  \n",
       "5            12  http://t.co/KnBv2YtNWc @qz @TaraSwart @vivian_...  \n",
       "6            14                          THEY WOULD GET TAKEN DOWN  \n",
       "7            14                         BUT BY THE FANS REAL QUICK  \n",
       "8            15                            : The incident happened  \n",
       "9            15                             http://t.co/RVMMuT3GvC  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_processed_training[supervised_processed_training['label']==1].iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# write \n",
    "print(len(supervised_processed_training[(supervised_processed_training['sentence_2']==supervised_processed_training['sentence_2a'])&\n",
    "                               supervised_processed_training['label']==1]))\n",
    "\n",
    "# write \n",
    "print(len(supervised_processed_test[(supervised_processed_test['sentence_2']==supervised_processed_test['sentence_2a'])&\n",
    "                               supervised_processed_test['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_processed_training.iloc[:,0:3].to_csv('../data/fine_tune_training_dataset.csv',index=False)\n",
    "supervised_processed_test.iloc[:,0:3].to_csv('../data/fine_tune_test_dataset.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
